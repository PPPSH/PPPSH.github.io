---
layout: single
title: "Flume - Kafka 로컬 설치 및 실습 "
categories: kafka
tag: [kafka, flume]
#toc: true 
author_profile: false
sidebar:
    nav: "docs"
---

##  Flume - Kafka 로컬 설치 및 실습 

**빅데이터 파일럿 프로젝트 강의** 정리를 하며 클라우데라 플랫폼 위에서만 실습을 진행했다면

실제 mac os 환경에서 flume과 kafka를 설치해서 실습해 보고자 이 포스트를 작성함.

<br>

간단하게 구상한 개요는

```
Log generator 로그 생성 (한 파일에 누적적재 like 실시간 ) 

-> 플럼수집 -> 카프카Sink로 전달 

-> 카프카 프로듀서 -> 토픽 -> 컨슈머 콘솔로 확인까지만
```

일단,  플럼 설치 및 간단한 구현에 초점을 둠

---

<br>





#### kafka 기본 명령어

```bash
#0. 주키퍼 실행 
bin/zookeeper-server-start.sh config/zookeeper.properties

#1. 브로서 실행
bin/kafka-server-start.sh config/server.

#2. Topic 생성 (파티션 개수, 복제개수 등 부가 옵션 가능)
bin/kafka-tipics.sh --create --bootstrap-server my-kafka:9092 --topic hello.kafka 


#2-1. Topic List 확인
bin/kafka-topics.sh --bootstrap-server. localhost:9092 --list

#3. Producer 생성
bin/kafka-console-producer.sh --bootstrap-server my-kafka:9092 --topic hello.kafka

#4. Consumer 생성 (--from-beginnig #처음부터 읽어오기)
bin/kafka-console-consumer.sh --bootstrap-server my-kafka:9092 --topic hello.kafka


```





## 플럼 실습

#### 1. conf.properties  환경변수 설정 (실시간 data 건 )

```bash
SmartCar_Agent.sources  = SmartCarInfo_SpoolSource DriverCarInfo_TailSource
SmartCar_Agent.channels = SmartCarInfo_Channel DriverCarInfo_Channel
SmartCar_Agent.sinks    = SmartCarInfo_LoggerSink DriverCarInfo_KafkaSink

SmartCar_Agent.sources.DriverCarInfo_TailSource.type = exec


#로그발생기에서 생긴 실시간 결과 log 파일 =  flume이 계속 바라보는 파일 
SmartCar_Agent.sources.DriverCarInfo_TailSource.command = tail -F /Users/ben/Dev/bigdata_PP/bigdata2nd-master/CH02/driver-realtime-log/SmartCarDriverInfo.log  

SmartCar_Agent.sources.DriverCarInfo_TailSource.restart = true
SmartCar_Agent.sources.DriverCarInfo_TailSource.batchSize = 1000

SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors = filterInterceptor2

SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.type = regex_filter
SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.regex = ^\\d{14}
SmartCar_Agent.sources.DriverCarInfo_TailSource.interceptors.filterInterceptor2.excludeEvents = false

#sink로 사용될 kafka 기본 정보 
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.type = org.apache.flume.sink.kafka.KafkaSink
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.topic = hello
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.brokerList = my-kafka:9092
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.requiredAcks = 1
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.batchSize = 1000

SmartCar_Agent.channels.DriverCarInfo_Channel.type = memory
SmartCar_Agent.channels.DriverCarInfo_Channel.capacity= 100000
SmartCar_Agent.channels.DriverCarInfo_Channel.transactionCapacity = 10000

SmartCar_Agent.sources.DriverCarInfo_TailSource.channels = DriverCarInfo_Channel
SmartCar_Agent.sinks.DriverCarInfo_KafkaSink.channel = DriverCarInfo_Channel
```



#### 2. flume 실행

```bash
bin/flume-ng agent --conf conf --conf-file conf/flume-conf.properties --name SmartCar_Agent -Dflume.root.logger=INFO,console
```

conf/ 폴더 내에 <u>**flume-conf.properties.template**</u> 복사 수정 해서 사용 

(실시간.log파일에 정보를  kafka consumer에서 실시간 받아오는것 까지 실습 ) 





#### 3. 결과 확인

```bash
bin/kafka-console-consumer.sh --bootstrap-server my-kafka:9092 --topic hello
```

위에 sink로 hello 라는 토픽에 컨슈머 만들어서 확인 

![kafka_result1](../../images/2022-08-16-카프카플럼로컬실습/kafka_result1.png)



( batch data 는 나중에 ... 플럼의 기본 로그가 어디 떨궈지는 확인이 안됨... 원래는 /var/log ... 있어야 하는데  )







